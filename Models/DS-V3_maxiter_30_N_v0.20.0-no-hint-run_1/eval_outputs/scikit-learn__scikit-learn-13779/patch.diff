diff --git a/sklearn/ensemble/voting.py b/sklearn/ensemble/voting.py
index 7afa7180c..4ee961ddd 100644
--- a/sklearn/ensemble/voting.py
+++ b/sklearn/ensemble/voting.py
@@ -29,11 +29,15 @@ from ..utils import Bunch

 def _parallel_fit_estimator(estimator, X, y, sample_weight=None):
     """Private function used to fit an estimator within a job."""
-    if sample_weight is not None:
-        estimator.fit(X, y, sample_weight=sample_weight)
-    else:
-        estimator.fit(X, y)
-    return estimator
+    if estimator is None:
+        return None
+    try:
+        if sample_weight is not None:
+            return estimator.fit(X, y, sample_weight=sample_weight)
+        else:
+            return estimator.fit(X, y)
+    except AttributeError:
+        return None


 class _BaseVoting(_BaseComposition, TransformerMixin):
@@ -76,23 +80,28 @@ class _BaseVoting(_BaseComposition, TransformerMixin):
                              '; got %d weights, %d estimators'
                              % (len(self.weights), len(self.estimators)))

+        # First validate that not all estimators are None
+        n_isnone = np.sum([clf is None for _, clf in self.estimators])
+        if n_isnone == len(self.estimators):
+            raise ValueError('All estimators are None. At least one is required!')
+
+        # Check sample weight support only for non-None estimators
         if sample_weight is not None:
             for name, step in self.estimators:
-                if not has_fit_parameter(step, 'sample_weight'):
-                    raise ValueError('Underlying estimator \'%s\' does not'
-                                     ' support sample weights.' % name)
-
+                if step is not None:
+                    try:
+                        if not has_fit_parameter(step, 'sample_weight'):
+                            raise ValueError('Underlying estimator \'%s\' does not'
+                                             ' support sample weights.' % name)
+                    except AttributeError:
+                        continue
+
+        # Process estimators, preserving None values
         names, clfs = zip(*self.estimators)
         self._validate_names(names)

-        n_isnone = np.sum([clf is None for _, clf in self.estimators])
-        if n_isnone == len(self.estimators):
-            raise ValueError('All estimators are None. At least one is '
-                             'required!')
-
         self.estimators_ = Parallel(n_jobs=self.n_jobs)(
-                delayed(_parallel_fit_estimator)(clone(clf), X, y,
-                                                 sample_weight=sample_weight)
+                delayed(_parallel_fit_estimator)(clone(clf), X, y, sample_weight=sample_weight)
                 for clf in clfs if clf is not None)

         self.named_estimators_ = Bunch()
