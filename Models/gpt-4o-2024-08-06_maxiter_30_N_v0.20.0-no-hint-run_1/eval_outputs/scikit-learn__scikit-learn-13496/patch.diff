diff --git a/doc/modules/outlier_detection.rst b/doc/modules/outlier_detection.rst
index b27b0c8a5..fedb5ae04 100644
--- a/doc/modules/outlier_detection.rst
+++ b/doc/modules/outlier_detection.rst
@@ -239,6 +239,8 @@ Random partitioning produces noticeably shorter paths for anomalies.
 Hence, when a forest of random trees collectively produce shorter path
 lengths for particular samples, they are highly likely to be anomalies.

+The `warm_start` parameter can be used to add more trees to the ensemble without starting from scratch, which can be useful for large datasets or when computational resources are limited.
+
 The implementation of :class:`ensemble.IsolationForest` is based on an ensemble
 of :class:`tree.ExtraTreeRegressor`. Following Isolation Forest original paper,
 the maximum depth of each tree is set to :math:`\lceil \log_2(n) \rceil` where
diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py
index 8a1bd3625..d640b5337 100644
--- a/sklearn/ensemble/iforest.py
+++ b/sklearn/ensemble/iforest.py
@@ -117,6 +117,11 @@ class IsolationForest(BaseBagging, OutlierMixin):
         If None, the random number generator is the RandomState instance used
         by `np.random`.

+    warm_start : bool, optional (default=False)
+        When set to ``True``, reuse the solution of the previous call to fit
+        and add more estimators to the ensemble, otherwise, just fit a whole
+        new forest. See :term:`the Glossary <warm_start>`.
+
     verbose : int, optional (default=0)
         Controls the verbosity of the tree building process.

@@ -170,6 +175,7 @@ class IsolationForest(BaseBagging, OutlierMixin):
                  contamination="legacy",
                  max_features=1.,
                  bootstrap=False,
+                 warm_start=False,
                  n_jobs=None,
                  behaviour='old',
                  random_state=None,
@@ -185,6 +191,7 @@ class IsolationForest(BaseBagging, OutlierMixin):
             n_estimators=n_estimators,
             max_samples=max_samples,
             max_features=max_features,
+            warm_start=warm_start,
             n_jobs=n_jobs,
             random_state=random_state,
             verbose=verbose)
diff --git a/test_warm_start_isolation_forest.py b/test_warm_start_isolation_forest.py
new file mode 100644
index 000000000..cf158b744
--- /dev/null
+++ b/test_warm_start_isolation_forest.py
@@ -0,0 +1,28 @@
+from sklearn.ensemble import IsolationForest
+from sklearn.datasets import make_classification
+
+# Create a simple dataset
+X, _ = make_classification(n_samples=100, n_features=20, random_state=42)
+
+# Initialize IsolationForest with warm_start=True
+iso_forest = IsolationForest(n_estimators=50, warm_start=True, random_state=42)
+
+# Fit the model with the initial number of estimators
+iso_forest.fit(X)
+
+# Store the number of estimators after the first fit
+initial_estimators = len(iso_forest.estimators_)
+print(f"Initial number of estimators: {initial_estimators}")
+
+# Increase the number of estimators
+iso_forest.set_params(n_estimators=100)
+
+# Fit the model again
+iso_forest.fit(X)
+
+# Store the number of estimators after the second fit
+final_estimators = len(iso_forest.estimators_)
+print(f"Final number of estimators: {final_estimators}")
+
+# Check if more estimators were added
+assert final_estimators > initial_estimators, "Warm start did not add more estimators!"
\ No newline at end of file
